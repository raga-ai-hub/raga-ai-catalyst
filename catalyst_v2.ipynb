{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEaiCalovMxx",
        "outputId": "fdb272ed-f165-4bfd-e2fe-24c9ca269a96"
      },
      "outputs": [],
      "source": [
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "import os\n",
        "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmJ7EJpZv1K2",
        "outputId": "a98662c9-28f8-4fbc-ea81-9fc557db25d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/siddharthakosti/anaconda3/envs/agentic_tracing/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "INFO:httpx:HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token(s) set successfully\n"
          ]
        }
      ],
      "source": [
        "from ragaai_catalyst import RagaAICatalyst\n",
        "\n",
        "catalyst = RagaAICatalyst(\n",
        "    access_key=\"access_key\",\n",
        "    secret_key=\"secret_key\",\n",
        "    base_url=\"base_url\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# project management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Chatbot', 'Text2SQL', 'Q/A', 'Code Generation', 'Others']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "catalyst.project_use_cases()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# catalyst.list_projects()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# project_name = 'chat_demo_sk_v1'\n",
        "project_name = \"prompt_metric_dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# project = catalyst.create_project(\n",
        "#     project_name=project_name,\n",
        "#     usecase=\"Others\" #default usecase Q/A print usecase \n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tracer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragaai_catalyst import Tracer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer_dataset_name = \"16dec_tarce_langchain_v2_with_AT\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer = Tracer(\n",
        "    project_name=project_name,\n",
        "    dataset_name=tracer_dataset_name,\n",
        "    metadata={\"key1\": \"value1\", \"key2\": \"value2\"},\n",
        "    tracer_type=\"langchain\",\n",
        "    pipeline={\n",
        "        \"llm_model\": \"gpt-4o-mini\",\n",
        "        \"vector_store\": \"faiss\",\n",
        "        \"embed_model\": \"text-embedding-ada-002\",\n",
        "    }\n",
        ")\n",
        "tracer.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install langchain-openai langchain-chroma langchain-community pypdf -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from opentelemetry.trace import SpanKind\n",
        "source_doc_path = \"/Users/siddharthakosti/Downloads/catalyst_error_handling/catalyst_v2/catalyst_v2_new_1/data/ai_document_061023_2.pdf\"\n",
        "\n",
        "# Initialize necessary variables\n",
        "retriever = None\n",
        "loaded_doc = None\n",
        "def load_document(source_doc_path):\n",
        "\n",
        "    try:\n",
        "        loader = PyPDFLoader(source_doc_path)\n",
        "        pages = loader.load_and_split()\n",
        "        embeddings = OpenAIEmbeddings()\n",
        "        vectorstore = Chroma.from_documents(pages, embeddings)\n",
        "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "        print(\"Document loaded and processed.\")\n",
        "        return retriever\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading the document: {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_response(retriever, query):\n",
        "\n",
        "    try:\n",
        "        # llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "        llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "        template = \"\"\"\n",
        "            You are a helpful AI assistant. Answer based on the context provided.\n",
        "            context: {context}\n",
        "            input: {input}\n",
        "            answer:\n",
        "            \"\"\"\n",
        "        prompt = PromptTemplate.from_template(template)\n",
        "        combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
        "        retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
        "        response = retrieval_chain.invoke({\"input\": query})\n",
        "        print(response[\"answer\"])\n",
        "        return response[\"answer\"]\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while generating the response: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_document(source_doc_path, loaded_doc, query):\n",
        "    try:\n",
        "        if loaded_doc != source_doc_path:\n",
        "            retriever = load_document(source_doc_path)\n",
        "            if retriever is None:\n",
        "                return \"Failed to load document.\"\n",
        "            loaded_doc = source_doc_path\n",
        "        else:\n",
        "            print(\"Using cached document retriever.\")\n",
        "        response = generate_response(retriever, query)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(f\"An overall error occurred: {e}\")\n",
        "        return \"An error occurred during the document processing.\"\n",
        "\n",
        "# query = \"What paper targets to solve, tell in 20 words?\"\n",
        "# response = process_document(source_doc_path, loaded_doc, query)\n",
        "\n",
        "# query = \"What paper aims to resolve, tell in 20 words?\"\n",
        "# response = process_document(source_doc_path, loaded_doc, query)\n",
        "\n",
        "query = \"What is this paper about, tell in 20 words?\"\n",
        "response = process_document(source_doc_path, loaded_doc, query)\n",
        "\n",
        "query = \"What is the domain of paper, tell in 10 words?\"\n",
        "response = process_document(source_doc_path, loaded_doc, query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## llamaindex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragaai_catalyst import Tracer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer_dataset_name = \"16dec_trace_llamaindex_v3_with_AT\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer = Tracer(\n",
        "    project_name=project_name,\n",
        "    dataset_name=tracer_dataset_name,\n",
        "    metadata={\"key1\": \"value1\", \"key2\": \"value2\"},\n",
        "    tracer_type=\"llamaindex\",\n",
        "    pipeline={\n",
        "        \"llm_model\": \"gpt-4o-mini\",\n",
        "        \"vector_store\": \"faiss\",\n",
        "        \"embed_model\": \"text-embedding-ada-002\",\n",
        "    }\n",
        ").start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, Settings, Document\n",
        "from llama_index.readers.file import PDFReader\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "# Initialize necessary variables\n",
        "retriever = None\n",
        "loaded_doc = None\n",
        "index = None\n",
        "\n",
        "def load_document(source_doc_path):\n",
        "    \"\"\"\n",
        "    Load and index the document using LlamaIndex\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize LLM and embedding model\n",
        "        Settings.llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "        Settings.embed_model = OpenAIEmbedding()\n",
        "        \n",
        "        \n",
        "        # Load PDF document\n",
        "        reader = PDFReader()\n",
        "        docs = reader.load_data(source_doc_path)\n",
        "        \n",
        "        # Create documents with metadata\n",
        "        documents = [\n",
        "            Document(text=doc.text, metadata={\"source\": source_doc_path})\n",
        "            for doc in docs\n",
        "        ]\n",
        "        \n",
        "        # Create vector store index\n",
        "        global index\n",
        "        index = VectorStoreIndex.from_documents(documents)\n",
        "        \n",
        "        # Create retriever (to maintain similar interface)\n",
        "        retriever = index.as_retriever(similarity_top_k=5)\n",
        "        \n",
        "        logger.info(\"Document loaded and processed.\")\n",
        "        return retriever\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred while loading the document: {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_response(retriever, query):\n",
        "    \"\"\"\n",
        "    Generate response for the given query using LlamaIndex\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if index is None:\n",
        "            logger.error(\"Index not initialized. Please load document first.\")\n",
        "            return None\n",
        "        \n",
        "        # Create query engine\n",
        "        query_engine = index.as_query_engine(\n",
        "            response_mode=\"compact\"\n",
        "        )\n",
        "        \n",
        "        # Generate response\n",
        "        response = query_engine.query(query)\n",
        "        \n",
        "        logger.info(\"Response generated successfully\")\n",
        "        return str(response)\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred while generating the response: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_document(source_doc_path, loaded_doc, query):\n",
        "    \"\"\"\n",
        "    Process document and generate response using LlamaIndex\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if we need to load a new document\n",
        "        if loaded_doc != source_doc_path:\n",
        "            retriever = load_document(source_doc_path)\n",
        "            if retriever is None:\n",
        "                return \"Failed to load document.\"\n",
        "            loaded_doc = source_doc_path\n",
        "        else:\n",
        "            logger.info(\"Using cached document retriever.\")\n",
        "        \n",
        "        # Generate response\n",
        "        response = generate_response(retriever, query)\n",
        "        if response is None:\n",
        "            return \"Failed to generate response.\"\n",
        "            \n",
        "        return response\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"An overall error occurred: {e}\")\n",
        "        return \"An error occurred during the document processing.\"\n",
        "    \n",
        "\n",
        "\n",
        "source_doc_path = \"/Users/siddharthakosti/Downloads/catalyst_error_handling/catalyst_v2/catalyst_v2_new_1/data/ai_document_061023_2.pdf\"\n",
        "\n",
        "# Process a query\n",
        "# query = \"What is this paper about?\"\n",
        "# response = process_document(source_doc_path, None, query)\n",
        "# print(f\"Response: {response}\")\n",
        "\n",
        "query = \"Give 10 words summary of the paper?\"\n",
        "response = process_document(source_doc_path, None, query)\n",
        "print(f\"Response: {response}\")\n",
        "\n",
        "# query = \"What is the main topic of the paper?\"\n",
        "# response = process_document(source_doc_path, None, query)\n",
        "# print(f\"Response: {response}\")\n",
        "\n",
        "# query = \"What is the aim of the paper, in 10 words?\"\n",
        "# response = process_document(source_doc_path, None, query)\n",
        "# # print(f\"Response: {response}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer.get_upload_status()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AgenticTracer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragaai_catalyst import Tracer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer_dataset_name = \"16dec_trace_agentictrace_v2_1_with_AT\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<ragaai_catalyst.tracers.tracer.Tracer at 0x28827ca00>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tracer = Tracer(\n",
        "    project_name=project_name,\n",
        "    dataset_name=tracer_dataset_name,\n",
        "    metadata={\"key1\": \"value1\", \"key2\": \"value2\"},\n",
        "    tracer_type=\"abc\",\n",
        "    pipeline={\n",
        "        \"llm_model\": \"gpt-4o-mini\",\n",
        "        \"vector_store\": \"faiss\",\n",
        "        \"embed_model\": \"text-embedding-ada-002\",\n",
        "    }\n",
        ")\n",
        "tracer.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TravelAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from openai import OpenAI\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "import sys\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tracer.trace_llm(name=\"llm_call\")\n",
        "def llm_call(prompt, max_tokens=512, model=\"gpt-3.5-turbo\"):\n",
        "    client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "# Tools outside agents\n",
        "@tracer.trace_tool(name=\"weather_tool\")\n",
        "def weather_tool(destination):\n",
        "\n",
        "    api_key = os.environ.get(\"OPENWEATHERMAP_API_KEY\")\n",
        "    base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n",
        "\n",
        "    params = {\"q\": destination, \"appid\": api_key, \"units\": \"metric\"}\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        weather_description = data[\"weather\"][0][\"description\"]\n",
        "        temperature = data[\"main\"][\"temp\"]\n",
        "\n",
        "        return f\"{weather_description.capitalize()}, {temperature:.1f}Â°C\"\n",
        "    except requests.RequestException:\n",
        "        return \"Weather data not available.\"\n",
        "\n",
        "\n",
        "@tracer.trace_tool(name=\"currency_converter_tool\")\n",
        "def currency_converter_tool(amount, from_currency, to_currency):\n",
        "    api_key = os.environ.get(\"EXCHANGERATE_API_KEY\")\n",
        "    base_url = f\"https://v6.exchangerate-api.com/v6/{api_key}/pair/{from_currency}/{to_currency}\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if data[\"result\"] == \"success\":\n",
        "            rate = data[\"conversion_rate\"]\n",
        "            return amount * rate\n",
        "        else:\n",
        "            return None\n",
        "    except requests.RequestException:\n",
        "        return None\n",
        "\n",
        "\n",
        "@tracer.trace_tool(name=\"flight_price_estimator_tool\")\n",
        "def flight_price_estimator_tool(origin, destination):\n",
        "    # This is a mock function. In a real scenario, you'd integrate with a flight API.\n",
        "    api_key = os.environ.get(\"FLIGHT_API_KEY\")\n",
        "    # Implement actual API call here\n",
        "    return f\"Estimated price from {origin} to {destination}: $500-$1000\"\n",
        "\n",
        "\n",
        "# Agent with persona\n",
        "@tracer.trace_agent(name=\"itinerary_agent\")\n",
        "class ItineraryAgent:\n",
        "    def __init__(self, persona=\"Itinerary Agent\"):\n",
        "        self.persona = persona\n",
        "\n",
        "    def plan_itinerary(self, user_preferences, duration=3):\n",
        "        itinerary_prompt = f\"\"\"\n",
        "You are a travel expert named {self.persona}.\n",
        "Based on the following user preferences, create a {duration}-day travel itinerary.\n",
        "\n",
        "User Preferences:\n",
        "{user_preferences}\n",
        "\n",
        "Itinerary:\n",
        "\"\"\"\n",
        "        return llm_call(itinerary_prompt, max_tokens=512)\n",
        "\n",
        "\n",
        "# Main function\n",
        "\n",
        "@tracer.trace_agent(name=\"travel_agent\")\n",
        "def travel_agent():\n",
        "    print(\"Welcome to the Personalized Travel Planner!\\n\")\n",
        "\n",
        "    # Get user input\n",
        "    # user_input = input(\"Please describe your ideal vacation: \")\n",
        "    user_input = \"karela, 10 days, $100, nature\"\n",
        "\n",
        "    # Extract preferences\n",
        "    preferences_prompt = f\"\"\"\n",
        "Extract key travel preferences from the following user input:\n",
        "\"{user_input}\"\n",
        "\n",
        "Please provide the extracted information in this format:\n",
        "Destination:\n",
        "Activities:\n",
        "Budget:\n",
        "Duration (in days):\n",
        "\"\"\"\n",
        "    extracted_preferences = llm_call(preferences_prompt)\n",
        "    print(\"\\nExtracted Preferences:\")\n",
        "    print(extracted_preferences)\n",
        "\n",
        "    # Parse extracted preferences\n",
        "    preferences = {}\n",
        "    for line in extracted_preferences.split(\"\\n\"):\n",
        "        if \":\" in line:\n",
        "            key, value = line.split(\":\", 1)\n",
        "            preferences[key.strip()] = value.strip()\n",
        "\n",
        "    # Validate extracted preferences\n",
        "    required_keys = [\"Destination\", \"Activities\", \"Budget\", \"Duration (in days)\"]\n",
        "    if not all(key in preferences for key in required_keys):\n",
        "        print(\"\\nCould not extract all required preferences. Please try again.\")\n",
        "        return\n",
        "\n",
        "    # Fetch additional information\n",
        "    weather = weather_tool(preferences[\"Destination\"])\n",
        "    print(f\"\\nWeather in {preferences['Destination']}: {weather}\")\n",
        "\n",
        "    # origin = input(\"Please enter your departure city: \")\n",
        "    origin = \"delhi\"\n",
        "    flight_price = flight_price_estimator_tool(origin, preferences[\"Destination\"])\n",
        "    print(flight_price)\n",
        "\n",
        "    # Plan itinerary\n",
        "    itinerary_agent = ItineraryAgent()\n",
        "    itinerary = itinerary_agent.plan_itinerary(\n",
        "        extracted_preferences, int(preferences[\"Duration (in days)\"])\n",
        "    )\n",
        "    print(\"\\nPlanned Itinerary:\")\n",
        "    print(itinerary)\n",
        "\n",
        "    # Currency conversion\n",
        "    budget_amount = float(preferences[\"Budget\"].replace(\"$\", \"\").replace(\",\", \"\"))\n",
        "    converted_budget = currency_converter_tool(budget_amount, \"USD\", \"INR\")\n",
        "    if converted_budget:\n",
        "        print(f\"\\nBudget in INR: {converted_budget:.2f} INR\")\n",
        "    else:\n",
        "        print(\"\\nCurrency conversion not available.\")\n",
        "\n",
        "    # Generate travel summary\n",
        "    summary_prompt = f\"\"\"\n",
        "Summarize the following travel plan:\n",
        "\n",
        "Destination: {preferences['Destination']}\n",
        "Activities: {preferences['Activities']}\n",
        "Budget: {preferences['Budget']}\n",
        "Duration: {preferences['Duration (in days)']} days\n",
        "Itinerary: {itinerary}\n",
        "Weather: {weather}\n",
        "Flight Price: {flight_price}\n",
        "\n",
        "Travel Summary:\n",
        "\"\"\"\n",
        "    travel_summary = llm_call(summary_prompt, max_tokens=2048)\n",
        "    print(\"\\nTravel Summary:\")\n",
        "    print(travel_summary)\n",
        "\n",
        "# Main function to run the travel agent\n",
        "def main():\n",
        "    travel_agent()\n",
        "\n",
        "# Ensure the script runs only when executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    tracer.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieve unique file names\n",
        "unique_files = tracer.get_unique_files()\n",
        "\n",
        "# Print the results\n",
        "print(\"Unique files:\")\n",
        "for file in unique_files:\n",
        "    print(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### FinancialAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from textblob import TextBlob\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "# from agentneo import AgentNeo, Tracer, Evaluation\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize OpenAI API\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FinancialAnalysisSystem:\n",
        "    def __init__(self):\n",
        "        self.stock_data = {}\n",
        "        self.news_sentiment = {}\n",
        "        self.economic_indicators = {}\n",
        "\n",
        "    @tracer.trace_tool(name=\"fetch_stock_data\")\n",
        "    def fetch_stock_data(self, symbol):\n",
        "        return {\n",
        "            \"symbol\": symbol,\n",
        "            \"price\": round(random.uniform(50, 500), 2),\n",
        "            \"change\": round(random.uniform(-5, 5), 2),\n",
        "        }\n",
        "\n",
        "    @tracer.trace_tool(name=\"fetch_news_articles\")\n",
        "    def fetch_news_articles(self, company):\n",
        "        return [\n",
        "            f\"{company} announces new product line\",\n",
        "            f\"{company} reports quarterly earnings\",\n",
        "            f\"{company} faces regulatory scrutiny\",\n",
        "        ]\n",
        "\n",
        "    @tracer.trace_tool(name=\"analyze_sentiment\")\n",
        "    def analyze_sentiment(self, text):\n",
        "        return TextBlob(text).sentiment.polarity\n",
        "\n",
        "    @tracer.trace_tool(name=\"fetch_economic_indicators\")\n",
        "    def fetch_economic_indicators(self):\n",
        "        return {\n",
        "            \"gdp_growth\": round(random.uniform(-2, 5), 2),\n",
        "            \"unemployment_rate\": round(random.uniform(3, 10), 2),\n",
        "            \"inflation_rate\": round(random.uniform(0, 5), 2),\n",
        "        }\n",
        "\n",
        "    @tracer.trace_llm(name=\"analyze_market_conditions\")\n",
        "    def analyze_market_conditions(self, stock_data, sentiment, economic_indicators):\n",
        "        prompt = f\"\"\"\n",
        "        Analyze the following market conditions and provide a brief market outlook:\n",
        "        Stock: {stock_data['symbol']} at ${stock_data['price']} (change: {stock_data['change']}%)\n",
        "        News Sentiment: {sentiment}\n",
        "        Economic Indicators:\n",
        "        - GDP Growth: {economic_indicators['gdp_growth']}%\n",
        "        - Unemployment Rate: {economic_indicators['unemployment_rate']}%\n",
        "        - Inflation Rate: {economic_indicators['inflation_rate']}%\n",
        "        \"\"\"\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-4-0125-preview\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=150,\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "\n",
        "    @tracer.trace_llm(name=\"generate_investment_recommendation\")\n",
        "    def generate_investment_recommendation(self, market_outlook, risk_tolerance):\n",
        "        prompt = f\"\"\"\n",
        "        Based on the following market outlook and investor risk tolerance,\n",
        "        provide a specific investment recommendation:\n",
        "        Market Outlook: {market_outlook}\n",
        "        Investor Risk Tolerance: {risk_tolerance}\n",
        "        \"\"\"\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-4-0125-preview\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=200,\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "\n",
        "    @tracer.trace_agent(name=\"FinancialAdvisorAgent\")\n",
        "    def financial_advisor_agent(self, stock_symbol, risk_tolerance):\n",
        "        self.stock_data = self.fetch_stock_data(stock_symbol)\n",
        "        news_articles = self.fetch_news_articles(stock_symbol)\n",
        "        sentiment_scores = [self.analyze_sentiment(article) for article in news_articles]\n",
        "        self.news_sentiment = sum(sentiment_scores) / len(sentiment_scores)\n",
        "        self.economic_indicators = self.fetch_economic_indicators()\n",
        "        market_outlook = self.analyze_market_conditions(\n",
        "            self.stock_data, self.news_sentiment, self.economic_indicators\n",
        "        )\n",
        "        recommendation = self.generate_investment_recommendation(market_outlook, risk_tolerance)\n",
        "        return recommendation\n",
        "\n",
        "    def run_analysis(self, stock_symbol, risk_tolerance):\n",
        "        recommendation = self.financial_advisor_agent(stock_symbol, risk_tolerance)\n",
        "        print(f\"\\nAnalysis for {stock_symbol}:\")\n",
        "        print(f\"Stock Data: {self.stock_data}\")\n",
        "        print(f\"News Sentiment: {self.news_sentiment}\")\n",
        "        print(f\"Economic Indicators: {self.economic_indicators}\")\n",
        "        print(f\"\\nInvestment Recommendation:\\n{recommendation}\")\n",
        "        if \"buy\" in recommendation.lower():\n",
        "            self.execute_buy_order(stock_symbol)\n",
        "        elif \"sell\" in recommendation.lower():\n",
        "            self.execute_sell_order(stock_symbol)\n",
        "        else:\n",
        "            print(\"No action taken based on the current recommendation.\")\n",
        "\n",
        "    @tracer.trace_tool(name=\"execute_buy_order\")\n",
        "    def execute_buy_order(self, symbol):\n",
        "        print(f\"Executing buy order for {symbol}\")\n",
        "\n",
        "    @tracer.trace_tool(name=\"execute_sell_order\")\n",
        "    def execute_sell_order(self, symbol):\n",
        "        print(f\"Executing sell order for {symbol}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Analysis for AAPL:\n",
            "Stock Data: {'symbol': 'AAPL', 'price': 133.69, 'change': 0.13}\n",
            "News Sentiment: 0.04545454545454545\n",
            "Economic Indicators: {'gdp_growth': -0.16, 'unemployment_rate': 6.81, 'inflation_rate': 2.86}\n",
            "\n",
            "Investment Recommendation:\n",
            "Based on the provided market outlook and considering a moderate investor risk tolerance, a specific investment recommendation would be to consider a diversified approach that includes a partial allocation to Apple Inc. (AAPL) shares along with a mix of other assets to balance risk and potential returns. Here's the rationale and recommendation in detail:\n",
            "\n",
            "### Rationale\n",
            "\n",
            "1. **Apple's Stability and Growth Potential:** Apple's slight uptick and its position as a market leader within technology and consumer goods sectors suggest it has the stability and potential for growth. This makes it an appealing investment for those with moderate risk tolerance. Its influence on major indices and ability to innovate could contribute to future appreciation.\n",
            "\n",
            "2. **Moderate Risk Tolerance:** This level of risk tolerance implies a preference for a balanced portfolio that seeks both growth and income while protecting against large downdrafts. Investors would not be looking to take on high-risk, high-reward investments but would still be open to moderate equity exposure.\n",
            "\n",
            "### Investment Recommendation\n",
            "\n",
            "- **Partial\n",
            "No action taken based on the current recommendation.\n",
            "Trace saved to traces/da5c4e6d-4aad-456a-bbf7-6785fbc36bad.json\n",
            "Uploading traces...\n",
            "Traces uplaoded\n"
          ]
        }
      ],
      "source": [
        "# Create an instance of FinancialAnalysisSystem\n",
        "analysis_system = FinancialAnalysisSystem()\n",
        "\n",
        "# Run an analysis for Apple stock with moderate risk tolerance\n",
        "analysis_system.run_analysis(\"AAPL\", \"moderate\")\n",
        "\n",
        "# Stop the tracer when analysis is complete\n",
        "tracer.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/var/folders/zj/xlc36kl17z72q0hcmzqyvv6c0000gn/T/ipykernel_50980/2425267025.py']\n"
          ]
        }
      ],
      "source": [
        "traced_files = tracer.get_traced_files()\n",
        "print(traced_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/var/folders/zj/xlc36kl17z72q0hcmzqyvv6c0000gn/T/ipykernel_49803/2425267025.py']\n"
          ]
        }
      ],
      "source": [
        "# traced_files = tracer.get_traced_files()\n",
        "# print(traced_files)\n",
        "# ['/var/folders/zj/xlc36kl17z72q0hcmzqyvv6c0000gn/T/ipykernel_49803/2425267025.py']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tracer management"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### start tracer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragaai_catalyst import Tracer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer_dataset_name = \"langchain_16Dec_AT_vj\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "project_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer = Tracer(\n",
        "    project_name=project_name,\n",
        "    dataset_name=tracer_dataset_name,\n",
        "    metadata={\"key1\": \"value1\", \"key2\": \"value2\"},\n",
        "    tracer_type=\"abc\",\n",
        "    pipeline={\n",
        "        \"llm_model\": \"gpt-4o-mini\",\n",
        "        \"vector_store\": \"faiss\",\n",
        "        \"embed_model\": \"text-embedding-ada-002\",\n",
        "    }\n",
        ")\n",
        "tracer.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### langchain ex1 sdk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install langchain-openai langchain-chroma langchain-community pypdf -q\n",
        "\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from opentelemetry.trace import SpanKind\n",
        "source_doc_path = \"/Users/siddharthakosti/Downloads/catalyst_error_handling/catalyst_v2/catalyst_v2_new_1/data/ai_document_061023_2.pdf\"\n",
        "\n",
        "# Initialize necessary variables\n",
        "retriever = None\n",
        "loaded_doc = None\n",
        "def load_document(source_doc_path):\n",
        "\n",
        "    try:\n",
        "        loader = PyPDFLoader(source_doc_path)\n",
        "        pages = loader.load_and_split()\n",
        "        embeddings = OpenAIEmbeddings()\n",
        "        vectorstore = Chroma.from_documents(pages, embeddings)\n",
        "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "        print(\"Document loaded and processed.\")\n",
        "        return retriever\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading the document: {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_response(retriever, query):\n",
        "\n",
        "    try:\n",
        "        # llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "        llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "        template = \"\"\"\n",
        "            You are a helpful AI assistant. Answer based on the context provided.\n",
        "            context: {context}\n",
        "            input: {input}\n",
        "            answer:\n",
        "            \"\"\"\n",
        "        prompt = PromptTemplate.from_template(template)\n",
        "        combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
        "        retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
        "        response = retrieval_chain.invoke({\"input\": query})\n",
        "        print(response[\"answer\"])\n",
        "        return response[\"answer\"]\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while generating the response: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_document(source_doc_path, loaded_doc, query):\n",
        "    try:\n",
        "        if loaded_doc != source_doc_path:\n",
        "            retriever = load_document(source_doc_path)\n",
        "            if retriever is None:\n",
        "                return \"Failed to load document.\"\n",
        "            loaded_doc = source_doc_path\n",
        "        else:\n",
        "            print(\"Using cached document retriever.\")\n",
        "        response = generate_response(retriever, query)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(f\"An overall error occurred: {e}\")\n",
        "        return \"An error occurred during the document processing.\"\n",
        "\n",
        "query = \"What paper targets to solve, tell in 20 words?\"\n",
        "response = process_document(source_doc_path, loaded_doc, query)\n",
        "\n",
        "query = \"What paper aims to resolve, tell in 20 words?\"\n",
        "response = process_document(source_doc_path, loaded_doc, query)\n",
        "\n",
        "# query = \"What is this paper about?\"\n",
        "# # Process the document and get the response\n",
        "# response = process_document(source_doc_path, loaded_doc, query)\n",
        "\n",
        "# query = \"What is the domain of paper?\"\n",
        "# # Process the document and get the response\n",
        "# response = process_document(source_doc_path, loaded_doc, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### langchain ex 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize the chat model\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "# Create a simple prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"Question: {question}\\nAnswer: \"\n",
        ")\n",
        "\n",
        "# Create the chain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Ask a question\n",
        "question = \"What is the capital of France?\"\n",
        "response = chain.run(question)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### langchain ex3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize the language model\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "# Create a simple prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"Question: {question}\\nAnswer: \"\n",
        ")\n",
        "\n",
        "# Create the chain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Ask a question\n",
        "question = \"What is the capital of France?\"\n",
        "response = chain.run(question)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### langchian ex 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = LANGSMITH_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load, chunk and index the contents of the blog.\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "\n",
        "# Retrieve and generate using the relevant snippets of the blog.\n",
        "retriever = vectorstore.as_retriever()\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TI code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### stop tracer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer.get_upload_status()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### langchain-extraction json check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from extraction_logic_langchain import extract_final_trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "with open(\"/Users/siddharthakosti/Downloads/catalyst_error_handling/catalyst_v2/catalyst_v2_new_1/ragaai-catalyst/tracer_debug_app4.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "extract_final_trace(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extract_final_trace_new(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install langchain-openai langchain-chroma langchain-community pypdf -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "# from langchain_chroma import Chroma\n",
        "# # from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# from langchain_community.document_loaders import PyPDFLoader\n",
        "# from langchain.prompts import PromptTemplate\n",
        "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "# from langchain.chains import create_retrieval_chain\n",
        "# from opentelemetry.trace import SpanKind\n",
        "# source_doc_path = \"/Users/siddharthakosti/Downloads/catalyst_error_handling/catalyst_v2/ai document_061023_2.pdf\"\n",
        "\n",
        "# # Initialize necessary variables\n",
        "# retriever = None\n",
        "# loaded_doc = None\n",
        "# def load_document(source_doc_path):\n",
        "\n",
        "#     try:\n",
        "#         loader = PyPDFLoader(source_doc_path)\n",
        "#         pages = loader.load_and_split()\n",
        "#         embeddings = OpenAIEmbeddings()\n",
        "#         vectorstore = Chroma.from_documents(pages, embeddings)\n",
        "#         retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "#         print(\"Document loaded and processed.\")\n",
        "#         return retriever\n",
        "#     except Exception as e:\n",
        "#         print(f\"An error occurred while loading the document: {e}\")\n",
        "#         return None\n",
        "\n",
        "# def generate_response(retriever, query):\n",
        "\n",
        "#     try:\n",
        "#         # llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "#         llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "#         template = \"\"\"\n",
        "#             You are a helpful AI assistant. Answer based on the context provided.\n",
        "#             context: {context}\n",
        "#             input: {input}\n",
        "#             answer:\n",
        "#             \"\"\"\n",
        "#         prompt = PromptTemplate.from_template(template)\n",
        "#         combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
        "#         retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
        "#         response = retrieval_chain.invoke({\"input\": query})\n",
        "#         print(response[\"answer\"])\n",
        "#         return response[\"answer\"]\n",
        "#     except Exception as e:\n",
        "#         print(f\"An error occurred while generating the response: {e}\")\n",
        "#         return None\n",
        "\n",
        "# def process_document(source_doc_path, loaded_doc, query):\n",
        "#     try:\n",
        "#         if loaded_doc != source_doc_path:\n",
        "#             retriever = load_document(source_doc_path)\n",
        "#             if retriever is None:\n",
        "#                 return \"Failed to load document.\"\n",
        "#             loaded_doc = source_doc_path\n",
        "#         else:\n",
        "#             print(\"Using cached document retriever.\")\n",
        "#         response = generate_response(retriever, query)\n",
        "#         return response\n",
        "#     except Exception as e:\n",
        "#         print(f\"An overall error occurred: {e}\")\n",
        "#         return \"An error occurred during the document processing.\"\n",
        "\n",
        "# query = \"Tell me about the paper.\"\n",
        "# response = process_document(source_doc_path, loaded_doc, query)\n",
        "\n",
        "# query = \"What is the domain of the paper?\"\n",
        "# response = process_document(source_doc_path, loaded_doc, query)\n",
        "\n",
        "# query = \"What is the issue addressed in the paper?\"\n",
        "# response = process_document(source_doc_path, loaded_doc, query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tracer.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer.get_upload_status()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## llama-index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragaai_catalyst import Tracer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer_dataset_name = \"llamaI_16_dec_v1\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer = Tracer(\n",
        "    project_name=project_name,\n",
        "    dataset_name=tracer_dataset_name,\n",
        "    metadata={\"key1\": \"value1\", \"key2\": \"value2\"},\n",
        "    tracer_type=\"llamaindex\",\n",
        "    pipeline={\n",
        "        \"llm_model\": \"gpt-4o-mini\",\n",
        "        \"vector_store\": \"faiss\",\n",
        "        \"embed_model\": \"text-embedding-ada-002\",\n",
        "    }\n",
        ").start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### app 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import Settings, StorageContext, VectorStoreIndex\n",
        "from llama_index.readers.file import PDFReader\n",
        "from llama_index.core import SimpleDirectoryReader, Settings, StorageContext, VectorStoreIndex\n",
        "# Load a PDF document\n",
        "documents = SimpleDirectoryReader(\"/Users/siddharthakosti/Downloads/catalyst_error_handling/catalyst_v2/catalyst_v2_new/data\").load_data()\n",
        "\n",
        "Settings.chunk_size = 1024\n",
        "nodes = Settings.node_parser.get_nodes_from_documents(documents)\n",
        "storage_context = StorageContext.from_defaults()\n",
        "\n",
        "summary_index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
        "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
        "\n",
        "# Querying the indices\n",
        "list_query_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\", use_async=True)\n",
        "vector_query_engine = vector_index.as_query_engine(response_mode=\"tree_summarize\", use_async=True)\n",
        "\n",
        "response = list_query_engine.query(\"Why is customer service training important??\")\n",
        "response = list_query_engine.query(\"What is influences of the driving direction within the calculation of the pitting load-carrying capacity of bevel and hypoid gears?\")\n",
        "# response = list_query_engine.query(\"Where school district 1401 is located?\")\n",
        "# response = list_query_engine.query(\"Alan Turing definition fall under the category of?\")\n",
        "# print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### app1: sdk example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, ServiceContext, Document\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.readers.file import PDFReader\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from typing import Any, Dict, List, Optional\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "# Initialize necessary variables\n",
        "retriever = None\n",
        "loaded_doc = None\n",
        "index = None\n",
        "\n",
        "def load_document(source_doc_path):\n",
        "    \"\"\"\n",
        "    Load and index the document using LlamaIndex\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize LLM and embedding model\n",
        "        llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "        embed_model = OpenAIEmbedding()\n",
        "        \n",
        "        # Create service context\n",
        "        service_context = ServiceContext.from_defaults(\n",
        "            llm=llm,\n",
        "            embed_model=embed_model,\n",
        "        )\n",
        "        \n",
        "        # Load PDF document\n",
        "        reader = PDFReader()\n",
        "        docs = reader.load_data(source_doc_path)\n",
        "        \n",
        "        # Create documents with metadata\n",
        "        documents = [\n",
        "            Document(text=doc.text, metadata={\"source\": source_doc_path})\n",
        "            for doc in docs\n",
        "        ]\n",
        "        \n",
        "        # Create vector store index\n",
        "        global index\n",
        "        index = VectorStoreIndex.from_documents(\n",
        "            documents,\n",
        "            service_context=service_context\n",
        "        )\n",
        "        \n",
        "        # Create retriever (to maintain similar interface)\n",
        "        retriever = index.as_retriever(similarity_top_k=5)\n",
        "        \n",
        "        logger.info(\"Document loaded and processed.\")\n",
        "        return retriever\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred while loading the document: {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_response(retriever, query):\n",
        "    \"\"\"\n",
        "    Generate response for the given query using LlamaIndex\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if index is None:\n",
        "            logger.error(\"Index not initialized. Please load document first.\")\n",
        "            return None\n",
        "            \n",
        "        # Initialize LLM with callbacks\n",
        "        llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "        \n",
        "        # Create service context with callback manager\n",
        "        service_context = ServiceContext.from_defaults(\n",
        "            llm=llm\n",
        "        )\n",
        "        \n",
        "        # Create query engine\n",
        "        query_engine = index.as_query_engine(\n",
        "            response_mode=\"compact\",\n",
        "            service_context=service_context\n",
        "        )\n",
        "        \n",
        "        # Generate response\n",
        "        response = query_engine.query(query)\n",
        "        \n",
        "        logger.info(\"Response generated successfully\")\n",
        "        return str(response)\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred while generating the response: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_document(source_doc_path, loaded_doc, query):\n",
        "    \"\"\"\n",
        "    Process document and generate response using LlamaIndex\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if we need to load a new document\n",
        "        if loaded_doc != source_doc_path:\n",
        "            retriever = load_document(source_doc_path)\n",
        "            if retriever is None:\n",
        "                return \"Failed to load document.\"\n",
        "            loaded_doc = source_doc_path\n",
        "        else:\n",
        "            logger.info(\"Using cached document retriever.\")\n",
        "        \n",
        "        # Generate response\n",
        "        response = generate_response(retriever, query)\n",
        "        if response is None:\n",
        "            return \"Failed to generate response.\"\n",
        "            \n",
        "        return response\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"An overall error occurred: {e}\")\n",
        "        return \"An error occurred during the document processing.\"\n",
        "    \n",
        "\n",
        "\n",
        "source_doc_path = \"/Users/siddharthakosti/Downloads/langchain_callbacks/llama-index/ai document_061023_2.pdf\"\n",
        "\n",
        "# Process a query\n",
        "query = \"What is this paper about?\"\n",
        "response = process_document(source_doc_path, None, query)\n",
        "print(f\"Response: {response}\")\n",
        "\n",
        "query = \"Give 20 words summary of the paper?\"\n",
        "response = process_document(source_doc_path, None, query)\n",
        "print(f\"Response: {response}\")\n",
        "\n",
        "query = \"What is the main topic of the paper?\"\n",
        "response = process_document(source_doc_path, None, query)\n",
        "print(f\"Response: {response}\")\n",
        "\n",
        "query = \"What is the aim of the paper?\"\n",
        "response = process_document(source_doc_path, None, query)\n",
        "print(f\"Response: {response}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### app4 only openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "print(OpenAI.complete) \n",
        "llm = OpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    api_key=OPENAI_API_KEY,  # uses OPENAI_API_KEY env var by default\n",
        ")\n",
        "\n",
        "resp = llm.complete(\"Paul Graham is \")\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### app 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/Users/siddharthakosti/Downloads/catalyst_error_handling/catalyst_v2/catalyst_v2_new_1/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What did the author do growing up?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### app5: gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install llama-index-llms-gemini llama-index -q\n",
        "\n",
        "import os\n",
        "\n",
        "GOOGLE_API_KEY = \"\"  # add your GOOGLE API key here\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "from llama_index.llms.gemini import Gemini\n",
        "\n",
        "llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    # api_key=\"some key\",  # uses GOOGLE_API_KEY env var by default\n",
        ")\n",
        "\n",
        "resp = llm.complete(\"Write a poem about a magic backpack\")\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### app6: Groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install llama-index-llms-groq llama-index -q\n",
        "\n",
        "from llama_index.llms.groq import Groq\n",
        "\n",
        "llm = Groq(model=\"llama3-70b-8192\", api_key=\"your_api_key\")\n",
        "\n",
        "response = llm.complete(\"Explain the importance of low latency LLMs\")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### app7: Cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install llama-index-llms-openai llama-index-llms-cohere llama-index -q\n",
        "\n",
        "from llama_index.llms.cohere import Cohere\n",
        "\n",
        "api_key = \"Your api key\"\n",
        "resp = Cohere(api_key=api_key).complete(\"Paul Graham is \")\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer.get_upload_status()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### llama-index extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from extraction_logic_llama_index import extract_llama_index_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "with open(\"/Users/siddharthakosti/Downloads/catalyst_error_handling/catalyst_v2/catalyst_v2_new_1/ragaai-catalyst/saved_trace_query_2_20241121_141851.json\", 'r') as f:\n",
        "    data = json.load(f)\n",
        "extract_llama_index_data(data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "project_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tracer_dataset_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragaai_catalyst import Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation = Evaluation(project_name=project_name, \n",
        "                        dataset_name=tracer_dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation.list_metrics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics=[\n",
        "        {\"name\": \"Hallucination\", \"config\": {\"model\": \"gpt-4o-mini\", \"provider\":\"OpenAI\"}, \"column_name\":\"Hallucination_v1\"},\n",
        "        {\"name\": \"Faithfulness\", \"config\": {\"model\": \"gpt-4o-mini\", \"provider\":\"OpenAI\"}, \"column_name\":\"Faithfulness_v1\"}\n",
        "        ]\n",
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation.add_metrics(metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation.get_status()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation.get_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHZinSmzwH7d"
      },
      "source": [
        "# prompt slug error handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RswMU4rHv_2j"
      },
      "outputs": [],
      "source": [
        "from ragaai_catalyst import PromptManager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJUqRltQxeXt"
      },
      "source": [
        "## project name not found error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "6_9C-ONswL13",
        "outputId": "e8a12b36-5d90-4cfc-f532-b9ad06526888"
      },
      "outputs": [],
      "source": [
        "prompt_manager = PromptManager(\"prompt_metric_dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## list prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.list_prompts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFLkT_xyxiPQ"
      },
      "source": [
        "## prompt not found error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "Jb0CNzfAwRcI",
        "outputId": "1ae41f12-7af9-475c-b99e-92ad5acdbdab"
      },
      "outputs": [],
      "source": [
        "prompt_manager.list_prompt_versions(prompt_name=\"hall\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## list prompt versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.list_prompt_versions(prompt_name=\"Hallu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAgiSc0YxliJ"
      },
      "source": [
        "## prompt version not found error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.get_prompt(prompt_name=\"hall\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.get_prompt(version=\"v8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.get_prompt(prompt_name=\"Hallu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "mVkbArG5wTXD",
        "outputId": "75197c40-e91b-4732-d705-d41c979b34a9"
      },
      "outputs": [],
      "source": [
        "prompt = prompt_manager.get_prompt(prompt_name=\"newapiv1\")\n",
        "prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt.get_variables()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt.get_model_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "/Users/siddharthakosti/Downloads/catalyst_error_handling/catalyst_v2/catalyst_v2_new_1/catalyst_2.0.6_release_notes_sample.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt.get_prompt_content()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9ro-FWHxoDn"
      },
      "source": [
        "## error due to missing prompt variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt.compile(\n",
        "    query='What is the capital of France?',\n",
        "    llm_response=\"The capital of France is Paris.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_eBPjlsxx_0"
      },
      "source": [
        "## error due to incorrect variable type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt.compile(\n",
        "    query='What is the capital of France?',\n",
        "    llm_response=\"The capital of France is Paris.\",\n",
        "    context=123\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yckgj1jxtcC"
      },
      "source": [
        "## error due to extra variable provided"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt.compile(\n",
        "    query='What is the capital of France?',\n",
        "    llm_response=\"The capital of France is Paris.\",\n",
        "    context=\"France is a country in Western Europe. Its capital and largest city is Paris.\",\n",
        "    expected_response= \"ABC\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compiled_prompt = prompt.compile(\n",
        "    query='What is the capital of France?',\n",
        "    llm_response=\"The capital of France is Paris.\",\n",
        "    context=\"France is a country in Western Europe. Its capital and largest city is Paris.\")\n",
        "compiled_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to call OpenAI API\n",
        "import openai\n",
        "def get_openai_response(prompt):\n",
        "    client = openai.OpenAI()\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=prompt\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_openai_response(compiled_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## add new prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "import os\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "from ragaai_catalyst import RagaAICatalyst\n",
        "\n",
        "# catalyst = RagaAICatalyst(\n",
        "#     access_key=\"dn5OuL29uA0K6G2046ZG\",\n",
        "#     secret_key=\"Je5Kyg787f4Tdth9c75VBAen198L8KOihMTrhdYB\",\n",
        "#     base_url='http://15.206.202.3/api'\n",
        "#     # api_keys=OPENAI_API_KEY\n",
        "# )\n",
        "# os.environ[\"RAGAAI_CATALYST_BASE_URL\"] = \"http://15.206.202.3/api\"\n",
        "\n",
        "catalyst = RagaAICatalyst(\n",
        "    access_key=\"vTVxbWhzNq053gXX4lX2\",\n",
        "    secret_key=\"Mz39sOAqop4WDxKvzCxExpXN4N1g0WUp6m6N8xvo\",\n",
        "    api_keys={\"OPENAI_API_KEY\": OPENAI_API_KEY}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragaai_catalyst import PromptManager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "project_name = \"final_test_v1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragaai_catalyst import PromptManager\n",
        "prompt_manager = PromptManager(project_name=project_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.list_prompts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.list_prompt_versions(prompt_name=\"Hallu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_name = \"new_prompt_1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# content = prompt_manager.get_prompt(\"Hallu\").get_prompt_content()\n",
        "content = [{'content': 'You are an LLM evaluation tool. Given any Prompt, Context and Response you identify information and metrics such as Hallucination in response. You self check at least 5 times to make sure you are correct',\n",
        "  'role': 'system'},\n",
        " {'content': 'This is the context: {{context}}\\n           This is LLM Response: {{llm_response}}\\n           This is user query: {{query}}\\n\\n           You have to check if the LLM Response is hallucinating wrt to user query and context.\\n           LLM Response should be based on facts present only in Context.\\n           New claims, wrong information, made-up information, or typical correct answers but not present in context should be called hallucination.\\n           Verify 5 times before responding.\\n           Answer in True if it is hallucinationg or False otherwise and give descriptive reason why you believe this is the answer.\\n           Also mention reason_type if it is due to Contradictory wrt context, Incorrect response wrt query, Absence from context.\\n\\n           Return the answer in JSON format without any extra text.\\n',\n",
        "  'role': 'user'}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.add_new_prompt(prompt_name, content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "content = [{'content': 'text 2', 'role': 'system'},\n",
        "           {'content': 'text 2','role': 'user'}]\n",
        "content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.add_new_prompt(prompt_name, content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.add_new_prompt(prompt_name, content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.add_new_prompt(\"prompt_system\", [{'content': '', 'role': 'system'}])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.add_new_prompt(\"prompt_user\", [{'content': '', 'role': 'user'}])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.add_new_prompt(\"prompt_system_w_ar\", [{'content': '{abc} {{def}} {{defaul}}', 'role': 'system'}])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_content = [\n",
        "  {\n",
        "    \"content\": \"adding prompt system variable {{abc}} {{asd}}\",\n",
        "    \"role\": \"system\"\n",
        "  },\n",
        "  {\n",
        "    \"content\": \"adding prompt user variable {{abc}} {{def}}\",\n",
        "    \"role\": \"user\"\n",
        "  },\n",
        "  {\n",
        "    \"content\": \"this is assistant {{abc}} {{ghi}}\",\n",
        "    \"role\": \"assistant\"\n",
        "  }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "content_str = [item[\"content\"] for item in new_content]\n",
        "content_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "content_str = \". \".join(content_str)\n",
        "content_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[\n",
        "  {\n",
        "    \"content\": \" system content\",\n",
        "    \"role\": \"system\"\n",
        "  },\n",
        "  {\n",
        "    \"content\": \"user content\",\n",
        "    \"role\": \"user\"\n",
        "  }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_manager.add_new_prompt(prompt_name+'b', new_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## APO requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  Initial prompt (in single string)\n",
        "2.  Task description (in single string)\n",
        "3.  model \n",
        "4.  api_base\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Guardrails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "import os\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragaai_catalyst import RagaAICatalyst\n",
        "catalyst = RagaAICatalyst(\n",
        "    access_key=\"ypajeHE5e7Nh2O51V0ep\",\n",
        "    secret_key=\"R2ukns5DkA2rylrgJXVGSPiZ5D5Y9i7eSMD6CJn6\",\n",
        "    base_url=\"http://65.2.138.219/api\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "catalyst.list_projects()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "project_name = \"Test_SJ\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragaai_catalyst import GuardrailsManager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gdm = GuardrailsManager(project_name=project_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gdm.list_deployment_ids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gdm.get_deployment(70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gdm.list_guardrails()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gdm.list_fail_condition()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gdm.create_deployment(\"sk-sdk-v4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gdm.list_deployment_ids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "guardrails_config = {\"guardrailFailConditions\": [\"TIMEOUT\"],\n",
        "                     \"deploymentFailCondition\": \"ALL_FAIL\",\n",
        "                     \"alternateResponse\": \"This is alternate\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "guardrails = [\n",
        "    {\n",
        "      \"name\": \"pii_lte\",\n",
        "      \"type\": \"PII\",\n",
        "      \"threshold\": {\n",
        "        \"lte\": 0.23\n",
        "      },\n",
        "      \"isHighRisk\": True,\n",
        "      \"isActive\": True\n",
        "    },\n",
        "    {\n",
        "      \"name\": \"pii_gte\",\n",
        "      \"type\": \"PII\",\n",
        "      \"threshold\": {\n",
        "        \"gte\": 0.23\n",
        "      },\n",
        "      \"isHighRisk\": True,\n",
        "      \"isActive\": False\n",
        "    },\n",
        "    {\n",
        "      \"name\": \"pii_eq_1\",\n",
        "      \"type\": \"PII\",\n",
        "      \"threshold\": {\n",
        "        \"eq\": 0.23\n",
        "      },\n",
        "      \"isHighRisk\": False,\n",
        "      \"isActive\": True\n",
        "    },\n",
        "    {\n",
        "      \"name\": \"pii_eq_2\",\n",
        "      \"type\": \"PII\",\n",
        "      \"threshold\": {\n",
        "        \"eq\": 0.23\n",
        "      },\n",
        "      \"isHighRisk\": False,\n",
        "      \"isActive\": False\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"toxicity_demo\",\n",
        "        \"type\": \"Toxicity\",\n",
        "        \"threshold\": {\n",
        "            \"eq\": 0.5\n",
        "        },\n",
        "        \"isHighRisk\": False,\n",
        "        \"isActive\": True\n",
        "        }  \n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gdm.add_guardrails(guardrails, guardrails_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "guardrails_config = {}\n",
        "# print(guardrails_config[\"abc\"])\n",
        "guardrails_config.get(\"isActive\",False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "payload = json.dumps({\n",
        "  \"isActive\": True,\n",
        "  \"guardrailFailConditions\": [\n",
        "    \"FAIL\",\n",
        "    \"TIMEOUT\"\n",
        "  ],\n",
        "  \"deploymentFailCondition\": \"ONE_FAIL\",\n",
        "  \"failAction\": {\n",
        "    \"action\": \"ALTERNATE_RESPONSE\",\n",
        "    \"args\": \"{\\\"alternateResponse\\\": \\\"This is the Alternate Response\\\"}\"\n",
        "  },\n",
        "  \"guardrails\": [\n",
        "    {\n",
        "      \"name\": \"pii_lte\",\n",
        "      \"type\": \"PII\",\n",
        "      \"threshold\": {\n",
        "        \"lte\": 0.23\n",
        "      },\n",
        "      \"isHighRisk\": True,\n",
        "      \"isActive\": True\n",
        "    }\n",
        "  ]\n",
        "})\n",
        "payload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "url = \"http://13.200.236.233/api/guardrail/deployment/11/configure\"\n",
        "\n",
        "payload = json.dumps({\n",
        "  \"isActive\": True,\n",
        "  \"guardrailFailConditions\": [\n",
        "    \"FAIL\",\n",
        "    \"TIMEOUT\"\n",
        "  ],\n",
        "  \"deploymentFailCondition\": \"ONE_FAIL\",\n",
        "  \"failAction\": {\n",
        "    \"action\": \"ALTERNATE_RESPONSE\",\n",
        "    \"args\": \"{\\\"alternateResponse\\\": \\\"This is the Alternate Response\\\"}\"\n",
        "  },\n",
        "  \"guardrails\": [\n",
        "    {\n",
        "      \"name\": \"pii_lte\",\n",
        "      \"type\": \"PII\",\n",
        "      \"threshold\": {\n",
        "        \"lte\": 0.23\n",
        "      },\n",
        "      \"isHighRisk\": True,\n",
        "      \"isActive\": True\n",
        "    }\n",
        "  ]\n",
        "})\n",
        "headers = {\n",
        "  'Authorization': 'Bearer eyJhbGciOiJIUzUxMiJ9.eyJ3cml0ZVByb2plY3RJZHMiOlsxLDIsM10sInN1YiI6ImFkbWluQHJhZ2EiLCJvcmdOYW1lIjoiUmFnYS1BSSIsIm9yZ0RvbWFpbiI6InJhZ2EiLCJyb2xlcyI6WyJST0xFX1NVUEVSIl0sInVzZXJGdWxsTmFtZSI6IlRlc3QgVXNlciIsInVzZXJOYW1lIjoiYWRtaW5AcmFnYSIsInVzZXJJZCI6MSwib3JnSWQiOjEsInJlYWRQcm9qZWN0SWRzIjpbMSwyLDNdLCJleHAiOjE3Mjk4NDI2OTYsImlhdCI6MTcyOTc1NjI5NiwianRpIjoiMSJ9.9AkfqPD4ldR7r-tWPeo9fNTS3BaWKGB9FYRPMpopXmxpS6GIQBjvusDnNnJyVhuHKQvuFNcvsilLGfqWNdjPng',\n",
        "  'Content-Type': 'application/json',\n",
        "  'X-Project-Id': '2'\n",
        "}\n",
        "\n",
        "response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "payload = json.dumps({\n",
        "  \"isActive\": True,\n",
        "  \"guardrailFailConditions\": [\n",
        "    \"FAIL\",\n",
        "    \"TIMEOUT\"\n",
        "  ],\n",
        "  \"deploymentFailCondition\": \"ONE_FAIL\",\n",
        "  \"failAction\": {\n",
        "    \"action\": \"ALTERNATE_RESPONSE\",\n",
        "    \"args\": \"{\\\"alternateResponse\\\": \\\"This is the Alternate Response\\\"}\"\n",
        "  },\n",
        "  \"guardrails\": [\n",
        "    {\n",
        "      \"name\": \"pii_lte\",\n",
        "      \"type\": \"PII\",\n",
        "      \"threshold\": {\n",
        "        \"lte\": 0.23\n",
        "      },\n",
        "      \"isHighRisk\": True,\n",
        "      \"isActive\": True\n",
        "    }\n",
        "  ]\n",
        "})\n",
        "payload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'{\"isActive\": true, \"guardrailFailConditions\": [\"TIMEOUT\"], \"deploymentFailCondition\": \"ALL_FAIL\", \"failAction\": {\"action\": \"ALTERNATE_RESPONSE\", \"args\": \"{\\'alternateResponse\\': \\'This is alternate\\'}\"}, \"guardrails\": [{\"name\": \"pii_lte\", \"type\": \"PII\", \"isHighRisk\": true, \"isActive\": true, \"threshold\": {\"lte\": 0.23}}, {\"name\": \"pii_gte\", \"type\": \"PII\", \"isHighRisk\": true, \"isActive\": false, \"threshold\": {\"gte\": 0.23}}, {\"name\": \"pii_eq_1\", \"type\": \"PII\", \"isHighRisk\": false, \"isActive\": true, \"threshold\": {\"eq\": 0.23}}, {\"name\": \"pii_eq_2\", \"type\": \"PII\", \"isHighRisk\": false, \"isActive\": false, \"threshold\": {\"eq\": 0.23}}, {\"name\": \"toxicity_demo\", \"type\": \"Toxicity\", \"isHighRisk\": false, \"isActive\": true, \"threshold\": {\"eq\": 0.5}}]}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "original_list = [\n",
        "    {'PII': 'pii_gte'},\n",
        "    {'PII': 'pii_lte'},\n",
        "    {'PII': 'pii_eq_2'},\n",
        "    {'Toxicity': 'toxicity_demo'},\n",
        "    {'PII': 'pii_eq_1'}\n",
        "]\n",
        "\n",
        "# Extract values into new list\n",
        "new_list = [list(d.values())[0] for d in original_list]\n",
        "\n",
        "print(new_list)\n",
        "# ['pii_gte', 'pii_lte', 'pii_eq_2', 'toxicity_demo', 'pii_eq_1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list(original_list[0].values())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_condif = {\n",
        " \"model_name\": \"abc\",\n",
        " \"te,perature\": 0.6,\n",
        " \"max_token\": 10\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "url = \"http://13.200.11.66:4000/chat/completions\"\n",
        "\n",
        "payload = json.dumps(\n",
        "        {\n",
        "    \"model\": model_config.pop(\"model\"),\n",
        "    **model_config,\n",
        "\n",
        "    \"messages\": [\n",
        "        {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"you are analyst\"\n",
        "        },\n",
        "        {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"tell me the name of best indian player\"\n",
        "        }\n",
        "    ],\n",
        "    \"user_id\": 1\n",
        "    }\n",
        ")\n",
        "headers = {\n",
        "  'Content-Type': 'application/json',\n",
        "  'Content-Type': 'application/json'\n",
        "}\n",
        "\n",
        "response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## synthetic_diwakaer raised issue try"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dotenv\n",
        "dotenv.load_dotenv()\n",
        "import os\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragaai_catalyst import SyntheticDataGeneration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "synthetic_data_generation = SyntheticDataGeneration()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_file = \"/Users/siddharthakosti/Downloads/langchain_callbacks/ai document_061023_2.pdf\"\n",
        "text = synthetic_data_generation.process_document(input_data=text_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res_shape = 0\n",
        "while res_shape != 1:\n",
        "    issue_res = synthetic_data_generation.generate_qna(text, question_type ='mcq',model_config={\"provider\":\"gemini\",\"model\":\"gemini/gemini-1.5-flash\"},n=10)\n",
        "    res_shape = issue_res.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "issue_res.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "agentic_tracing",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
